{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ak.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO1spTu5n5Dxj8pyHKEPGoq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PoojaB29/nlp_assignment2/blob/main/ak.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NRcqdVIUlA00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd5dbbed-5a0a-483f-9769-7357a1fa249c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tweet = open(\"./file4\").read()\n",
        "def tokenize(text):\n",
        "  res = word_tokenize(text)\n",
        "  return res\n",
        "\n",
        "ans = tokenize(tweet)\n",
        "print(ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxzbzuT4SRfU",
        "outputId": "27f4b20a-004d-4edd-8b40-13a7c876a09a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "['@', 'marklobbezoo', 'Mooie', 'winnaar', '!', 'campanha', ':', 'vanessa', 'libera', 'para', 'stephany', '!', ',', 'OAISDJAID', 'RT', '@', 'duuuds_CONSEGUIMOS', '.', 'AE', 'BRASIL', '.', 'rs', 'Mydeadpony', 'http', ':', '//ff.im/-4yn2L', '@', 'aplusk', '@', 'marcydevanea', '@', 'gabiib', '@', 'franliveira', 'http', ':', '//weheartit.com/entry/620926', 'u', \"'ll\", 'definetly', 'like', 'it', 'HAHHAHAHAH', 'The', 'eternal', 'sunshine', 'of', 'the', 'spotless', 'mind', 'RT', '@', 'pedrocs', 'Para', 'quem', 'perdeu', 'os', 'Tweets', 'do', 'início', 'o', 'iPhone', '3GS', '32', 'GB', 'desbloqueado', 'custa', '1.054,99', 'eur', 'na', 'expansys', 'http', ':', '//is.gd/1hYFP', 'RT.Two', 'minute', 'Silence', 'for', 'the', 'victims', 'of', 'freedom', 'Anywhere', 'in', 'the', 'world', 'Tuesday', ',', 'June', '30', 'at', '12', 'PM', '#', 'iranelection', 'Just', 'changed', 'my', 'twitter', 'background', ',', 'check', 'it', 'out', '!', 'Found', 'it', 'at', 'http', ':', '//www.TwitterBackgrounds.com', 'More', 'Twilight', 'inspired', 'make-up', 'http', ':', '//tinyurl.com/le5gl9', 'ITS', 'GON', 'RAIN', 'Feira', 'Solidária', 'do', 'livro', 'escolar.Uma', 'campanha', 'para', 'ajudar', 'o', 'orçamento', 'de', 'algumas', 'famílias', 'no', 'início', 'do', 'ano', 'escolar', '.', 'Subam', 'ao', 'Meu', 'Mirante', '.', 'If', 'I', 'just', 'drink', 'ONE', 'FREAKIN', \"'\", 'CUP', 'of', 'water..', 'I', \"'ll\", 'be', 'peeing', 'clear', 'every', 'five', 'minutes', '.', 'D', '&', 'lt', ';', 'Adding', '8', 'new', 'free', '3', 'minute', 'movies', 'of', 'busty', 'hardcore', 'xxx', '-', 'http', ':', '//tinyurl.com/njwhoj', 'Woman', 'arrested', 'for', 'drunk', 'breast-feeding', 'http', ':', '//bit.ly/i0t8X', 'RT', '@', 'StarSports', 'Raptors', 'facing', 'free-agent', 'quandary', 'http', ':', '//bit.ly/zAPac', 'RT', '@', '1vs100XboxLIVE', ':', 'It', \"'s\", 'time', 'to', 'ask', 'the', 'question', ':', 'Are', 'you', 'ready', 'for', 'Superhard', '?', '37', 'very', 'tough', 'Qs', '.', '#', '1vs100', 'E.P', '.', 'tonight', ',', 'and', 'the', 'debut', 'of', '...', 'Adult-All', 'Ученые', 'исследовали', 'Камасутру', ':', 'Группа', 'специалистов', 'исследовали', 'на', 'практике', 'возможность', 'выполне..', 'http', ':', '//bit.ly/cvIvp', 'came', 'bk', 'to', 'the', 'school', ',', 'to', 'watch', 'another', 'movie', '!', 'listening', 'to', 'last', 'nights', 'conference', 'call', ',', 'so', 'many', 'people', 'were', 'on', 'we', 'couldnt', 'get', 'it', '!', '!', '!', '!', 'http', ':', '//bit.ly/qJSp4', '@', 'kathroom', 'Agree', 'the', 'temperature', ',', 'shame', 'about', 'the', 'predatory', 'followers', '!', 'Brings', 'global', 'warming', 'into', 'graphic', 'view', \"tho'\", '@', '_Sebastian_', 'so', 'what', 'is', 'gdgt', '?', 'And', 'why', 'should', 'I', 'care', '?', ';', '-', ')', 'Oregon', 'man', \"'s\", 'wallet', 'returns', 'after', '63', 'years', '(', 'AP', ')', 'http', ':', '//viigo.im/06AM', 'watching', 'Runs', 'House', 'on', '#', 'mtv', 'and', 'wondering', 'who', 'wears', 'a', 'fur', 'coat', 'to', 'bring', 'the', 'baby', 'to', 'gymboree', '?', '!', '?', 'love', 'it', '.', 'NA', 'Fileplanet', 'subscribers', '!', 'Get', 'your', '3rd', 'Aion', 'beta', 'weekend', 'keys', 'now', '!', 'http', ':', '//bit.ly/EJiut', 'The', 'Evolution', 'of', 'Media', 'Content', 'Distribution', ':', 'Circulation', '1.0', 'to', '...', 'http', ':', '//bit.ly/Hxsir', 'Healing', 'Philosophy', ':', 'Gentlemen', ',', 'gentleness', 'and', 'greatness', '...', 'http', ':', '//bit.ly/oaMrn', 'Eu', 'tambem', 'vi', '!', '!', '!', 'RT', '@', 'omalestafeito', ':', 'não', 'quero', 'ser', 'chato', ',', 'mas', 'pareceu-me', 'ver', 'a', 'Minsitra', 'da', 'Educação', 'fazer', 'um', 'manguito', 'http', ':', '//bit.ly/xzl94', 'OH', 'MY', 'FUCKING', 'GOD', '!', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tweet = open(\"./file4\").read()\n",
        "contractions_dict = {\"ain't\": \"are not\", \"'s\":\" is\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"‘cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\", \"I'm\": \"I am\", \"I've\": \"I have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"they'd\": \"they would\", \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
        " \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what've\": \"what have\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where've\": \"where have\",\n",
        " \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who've\": \"who have\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", \"it's\": \"it is\"}\n",
        "\n",
        "def clitics_handler(text, d):\n",
        "  l = {}\n",
        "  t = text.split()\n",
        "  for words in t:\n",
        "    # print(words)\n",
        "    if words in d:\n",
        "      l[words] = d[words]\n",
        "  return l\n",
        "\n",
        "print()\n",
        "result = clitics_handler(tweet, contractions_dict)\n",
        "print(\"Clitics:\")\n",
        "for i,j in result.items():\n",
        "  print(i, \" --> \", j)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHY7I-FcZ0kn",
        "outputId": "1876f76d-d758-4338-efcc-0fc045a5b8d8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "\n",
            "Clitics:\n",
            "I'll  -->  I will\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tweet = open(\"./file4\").read()\n",
        "tokens = tweet.split()\n",
        "def hash_user_ref(tokens):\n",
        "  l = []\n",
        "  for words in tokens:\n",
        "    if words[0] == \"@\":\n",
        "      w = re.search(\"^@.*\\w\", words)\n",
        "      l.append(w[0])\n",
        "    elif words[0] == \"#\":\n",
        "      w = re.search(\"^#.*\\w\", words)\n",
        "      l.append(w[0])\n",
        "  return l\n",
        "\n",
        "r = hash_user_ref(tokens)\n",
        "print()\n",
        "print(\"HashTags and User References:\")\n",
        "print(r) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl8ViKwFU62S",
        "outputId": "18247f02-84ea-4c0c-b2aa-257b0b2fe335"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "\n",
            "HashTags and User References:\n",
            "['@marklobbezoo', '@duuuds_CONSEGUIMOS', '@aplusk', '@marcydevanea', '@gabiib', '@franliveira', '@pedrocs', '#iranelection', '@StarSports', '@1vs100XboxLIVE', '#1vs100', '@kathroom', '@_Sebastian_', '#mtv', '@omalestafeito']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tweet = open(\"./file4\").read()\n",
        "smiley_dict = {\":D\" : 1, \":(\": 1, \";-\": 1}\n",
        "def catch_smileys(tokens):\n",
        "  l = []\n",
        "  for words in tokens:\n",
        "    if words in smiley_dict:\n",
        "      l.append(words)\n",
        "  return l\n",
        "\n",
        "print()\n",
        "a = catch_smileys(tokens)\n",
        "print(\"Smiley's:\")\n",
        "print(a)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSL2GxddVnbv",
        "outputId": "84fe6f8c-bc27-4ac8-8a16-f882ce2d6664"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "\n",
            "Smiley's:\n",
            "[]\n"
          ]
        }
      ]
    }
  ]
}